{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch\n!pip install tokenizers\n!pip install transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-22T16:54:31.014881Z","iopub.execute_input":"2023-05-22T16:54:31.015530Z","iopub.status.idle":"2023-05-22T16:55:06.785724Z","shell.execute_reply.started":"2023-05-22T16:54:31.015496Z","shell.execute_reply":"2023-05-22T16:55:06.784617Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.0.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.11.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.11.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: tokenizers in /opt/conda/lib/python3.10/site-packages (0.13.3)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.28.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.4)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.64.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.11.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.3.23)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.28.2)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2022.12.7)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"# Pre-training BERT on Youtube Comments Dataset","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nfrom transformers import BertTokenizer, BertForMaskedLM\nimport torch","metadata":{"execution":{"iopub.status.busy":"2023-05-22T16:59:11.485590Z","iopub.execute_input":"2023-05-22T16:59:11.485983Z","iopub.status.idle":"2023-05-22T16:59:22.503564Z","shell.execute_reply.started":"2023-05-22T16:59:11.485949Z","shell.execute_reply":"2023-05-22T16:59:22.502683Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"gb_comments_df = pd.read_csv('/kaggle/input/youtube-videos-title-description-comments/GBcomments.csv', on_bad_lines='skip')\ngb_comments_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-22T17:02:42.451745Z","iopub.execute_input":"2023-05-22T17:02:42.452620Z","iopub.status.idle":"2023-05-22T17:02:44.596724Z","shell.execute_reply.started":"2023-05-22T17:02:42.452585Z","shell.execute_reply":"2023-05-22T17:02:44.595817Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"      video_id                                       comment_text  likes  \\\n0  jt2OHQh0HoQ  It's more accurate to call it the M+ (1000) be...      0   \n1  jt2OHQh0HoQ              To be there with a samsung phone\\nüòÇüòÇüòÇ      1   \n2  jt2OHQh0HoQ  Thank gosh, a place I can watch it without hav...      0   \n3  jt2OHQh0HoQ  What happened to the home button on the iPhone...      0   \n4  jt2OHQh0HoQ  Power is the disease.¬† Care is the cure.¬† Keep...      0   \n\n   replies  \n0        0  \n1        0  \n2        0  \n3        0  \n4        0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>video_id</th>\n      <th>comment_text</th>\n      <th>likes</th>\n      <th>replies</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>jt2OHQh0HoQ</td>\n      <td>It's more accurate to call it the M+ (1000) be...</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>jt2OHQh0HoQ</td>\n      <td>To be there with a samsung phone\\nüòÇüòÇüòÇ</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>jt2OHQh0HoQ</td>\n      <td>Thank gosh, a place I can watch it without hav...</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>jt2OHQh0HoQ</td>\n      <td>What happened to the home button on the iPhone...</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>jt2OHQh0HoQ</td>\n      <td>Power is the disease.¬† Care is the cure.¬† Keep...</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"us_comments_df = pd.read_csv('/kaggle/input/youtube-videos-title-description-comments/UScomments.csv', on_bad_lines='skip')\nus_comments_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-22T17:02:51.636546Z","iopub.execute_input":"2023-05-22T17:02:51.636912Z","iopub.status.idle":"2023-05-22T17:02:53.785729Z","shell.execute_reply.started":"2023-05-22T17:02:51.636882Z","shell.execute_reply":"2023-05-22T17:02:53.784743Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/3472227992.py:1: DtypeWarning: Columns (2,3) have mixed types. Specify dtype option on import or set low_memory=False.\n  us_comments_df = pd.read_csv('/kaggle/input/youtube-videos-title-description-comments/UScomments.csv', on_bad_lines='skip')\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"      video_id                                       comment_text likes  \\\n0  XpVt6Z1Gjjo                  Logan Paul it's yo big day ‚ÄºÔ∏è‚ÄºÔ∏è‚ÄºÔ∏è     4   \n1  XpVt6Z1Gjjo  I've been following you from the start of your...     3   \n2  XpVt6Z1Gjjo                 Say hi to Kong and maverick for me     3   \n3  XpVt6Z1Gjjo                                MY FAN . attendance     3   \n4  XpVt6Z1Gjjo                                         trending üòâ     3   \n\n  replies  \n0       0  \n1       0  \n2       0  \n3       0  \n4       0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>video_id</th>\n      <th>comment_text</th>\n      <th>likes</th>\n      <th>replies</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>XpVt6Z1Gjjo</td>\n      <td>Logan Paul it's yo big day ‚ÄºÔ∏è‚ÄºÔ∏è‚ÄºÔ∏è</td>\n      <td>4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>XpVt6Z1Gjjo</td>\n      <td>I've been following you from the start of your...</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>XpVt6Z1Gjjo</td>\n      <td>Say hi to Kong and maverick for me</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>XpVt6Z1Gjjo</td>\n      <td>MY FAN . attendance</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>XpVt6Z1Gjjo</td>\n      <td>trending üòâ</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"comments_df = pd.concat([gb_comments_df, us_comments_df], axis=0)\nprint(\"Len of GB data - \", len(gb_comments_df))\nprint(\"Len of US data - \", len(us_comments_df))\nprint(\"Len of combined data - \", len(comments_df))\ncomments_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-22T17:02:58.424795Z","iopub.execute_input":"2023-05-22T17:02:58.425160Z","iopub.status.idle":"2023-05-22T17:02:58.548343Z","shell.execute_reply.started":"2023-05-22T17:02:58.425129Z","shell.execute_reply":"2023-05-22T17:02:58.547195Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Len of GB data -  718452\nLen of US data -  691400\nLen of combined data -  1409852\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"      video_id                                       comment_text likes  \\\n0  jt2OHQh0HoQ  It's more accurate to call it the M+ (1000) be...     0   \n1  jt2OHQh0HoQ              To be there with a samsung phone\\nüòÇüòÇüòÇ     1   \n2  jt2OHQh0HoQ  Thank gosh, a place I can watch it without hav...     0   \n3  jt2OHQh0HoQ  What happened to the home button on the iPhone...     0   \n4  jt2OHQh0HoQ  Power is the disease.¬† Care is the cure.¬† Keep...     0   \n\n  replies  \n0       0  \n1       0  \n2       0  \n3       0  \n4       0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>video_id</th>\n      <th>comment_text</th>\n      <th>likes</th>\n      <th>replies</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>jt2OHQh0HoQ</td>\n      <td>It's more accurate to call it the M+ (1000) be...</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>jt2OHQh0HoQ</td>\n      <td>To be there with a samsung phone\\nüòÇüòÇüòÇ</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>jt2OHQh0HoQ</td>\n      <td>Thank gosh, a place I can watch it without hav...</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>jt2OHQh0HoQ</td>\n      <td>What happened to the home button on the iPhone...</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>jt2OHQh0HoQ</td>\n      <td>Power is the disease.¬† Care is the cure.¬† Keep...</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"len(comments_df.comment_text.unique())","metadata":{"execution":{"iopub.status.busy":"2023-05-22T17:03:32.843587Z","iopub.execute_input":"2023-05-22T17:03:32.843960Z","iopub.status.idle":"2023-05-22T17:03:33.251005Z","shell.execute_reply.started":"2023-05-22T17:03:32.843929Z","shell.execute_reply":"2023-05-22T17:03:33.250152Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"622630"},"metadata":{}}]},{"cell_type":"code","source":"comments = comments_df.comment_text.unique()\ncomments[:5]","metadata":{"execution":{"iopub.status.busy":"2023-05-22T17:06:56.363010Z","iopub.execute_input":"2023-05-22T17:06:56.363386Z","iopub.status.idle":"2023-05-22T17:06:56.716331Z","shell.execute_reply.started":"2023-05-22T17:06:56.363356Z","shell.execute_reply":"2023-05-22T17:06:56.715228Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"array([\"It's more accurate to call it the M+ (1000) because the price is closer than calling it the X (10).\",\n       'To be there with a samsung phone\\\\nüòÇüòÇüòÇ',\n       'Thank gosh, a place I can watch it without having to be at HD... my speed doesn‚Äôt support HD',\n       'What happened to the home button on the iPhone X? *****Cough****copying Samsung******cough',\n       'Power is the disease.\\xa0 Care is the cure.\\xa0 Keep caring for yourself and others as best as you can.\\xa0 This is life.'],\n      dtype=object)"},"metadata":{}}]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Device: ', device.type)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T17:45:01.352621Z","iopub.execute_input":"2023-05-22T17:45:01.352988Z","iopub.status.idle":"2023-05-22T17:45:01.360280Z","shell.execute_reply.started":"2023-05-22T17:45:01.352957Z","shell.execute_reply":"2023-05-22T17:45:01.358692Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Device:  cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertForMaskedLM.from_pretrained('bert-base-uncased')\n\nmodel.to(device)\n\ntext = (\"After Abraham Lincoln won the November 1860 presidential \"\n        \"election on an anti-slavery platform, an initial seven \"\n        \"slave states declared their secession from the country \"\n        \"to form the Confederacy. War broke out in April 1861 \"\n        \"when secessionist forces attacked Fort Sumter in South \"\n        \"Carolina, just over a month after Lincoln's \"\n        \"inauguration.\")","metadata":{"execution":{"iopub.status.busy":"2023-05-22T17:47:31.142404Z","iopub.execute_input":"2023-05-22T17:47:31.142794Z","iopub.status.idle":"2023-05-22T17:47:37.814941Z","shell.execute_reply.started":"2023-05-22T17:47:31.142743Z","shell.execute_reply":"2023-05-22T17:47:37.813955Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"inputs = tokenizer(text, return_tensors='pt')","metadata":{"execution":{"iopub.status.busy":"2023-05-22T17:47:37.816697Z","iopub.execute_input":"2023-05-22T17:47:37.817053Z","iopub.status.idle":"2023-05-22T17:47:37.824157Z","shell.execute_reply.started":"2023-05-22T17:47:37.817027Z","shell.execute_reply":"2023-05-22T17:47:37.823188Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"inputs.keys()","metadata":{"execution":{"iopub.status.busy":"2023-05-22T17:47:37.825708Z","iopub.execute_input":"2023-05-22T17:47:37.826399Z","iopub.status.idle":"2023-05-22T17:47:37.835365Z","shell.execute_reply.started":"2023-05-22T17:47:37.826364Z","shell.execute_reply":"2023-05-22T17:47:37.834491Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"},"metadata":{}}]},{"cell_type":"code","source":"inputs['labels'] = inputs.input_ids.detach().clone()","metadata":{"execution":{"iopub.status.busy":"2023-05-22T17:47:37.838136Z","iopub.execute_input":"2023-05-22T17:47:37.838804Z","iopub.status.idle":"2023-05-22T17:47:37.843200Z","shell.execute_reply.started":"2023-05-22T17:47:37.838751Z","shell.execute_reply":"2023-05-22T17:47:37.842295Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"inputs.keys()","metadata":{"execution":{"iopub.status.busy":"2023-05-22T17:47:37.844616Z","iopub.execute_input":"2023-05-22T17:47:37.845278Z","iopub.status.idle":"2023-05-22T17:47:37.853906Z","shell.execute_reply.started":"2023-05-22T17:47:37.845244Z","shell.execute_reply":"2023-05-22T17:47:37.852821Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])"},"metadata":{}}]},{"cell_type":"markdown","source":"Masking 15% of the tokens","metadata":{}},{"cell_type":"code","source":"rand = torch.rand(inputs.input_ids.shape)\n# where the random array is less than 0.15, we set true\nmask_arr = rand < 0.15\nmask_arr","metadata":{"execution":{"iopub.status.busy":"2023-05-22T17:47:37.855203Z","iopub.execute_input":"2023-05-22T17:47:37.855997Z","iopub.status.idle":"2023-05-22T17:47:37.865526Z","shell.execute_reply.started":"2023-05-22T17:47:37.855964Z","shell.execute_reply":"2023-05-22T17:47:37.864426Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"tensor([[ True, False, False, False, False, False, False, False, False, False,\n         False, False,  True,  True,  True, False, False, False, False,  True,\n         False,  True, False, False,  True,  True, False, False, False, False,\n         False, False, False, False, False, False, False,  True, False, False,\n         False,  True, False, False, False, False, False, False, False, False,\n         False, False,  True, False, False, False, False, False, False,  True,\n          True, False]])"},"metadata":{}}]},{"cell_type":"markdown","source":"We don‚Äôt want to place a MASK token over other special tokens such as CLS or SEP tokens (101 and 102 respectively).\n\nSo, we need to add an additional condition. A check for positions containing the token ids 101 or 102.","metadata":{}},{"cell_type":"code","source":"(inputs.input_ids != 101) * (inputs.input_ids != 102)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T17:47:37.866782Z","iopub.execute_input":"2023-05-22T17:47:37.867441Z","iopub.status.idle":"2023-05-22T17:47:37.874808Z","shell.execute_reply.started":"2023-05-22T17:47:37.867409Z","shell.execute_reply":"2023-05-22T17:47:37.873817Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"tensor([[False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n          True, False]])"},"metadata":{}}]},{"cell_type":"code","source":"mask_arr = (rand < 0.15) * (inputs.input_ids != 101) * (inputs.input_ids != 102)\nmask_arr","metadata":{"execution":{"iopub.status.busy":"2023-05-22T17:47:37.876337Z","iopub.execute_input":"2023-05-22T17:47:37.876962Z","iopub.status.idle":"2023-05-22T17:47:37.886470Z","shell.execute_reply.started":"2023-05-22T17:47:37.876928Z","shell.execute_reply":"2023-05-22T17:47:37.885721Z"},"trusted":true},"execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"tensor([[False, False, False, False, False, False, False, False, False, False,\n         False, False,  True,  True,  True, False, False, False, False,  True,\n         False,  True, False, False,  True,  True, False, False, False, False,\n         False, False, False, False, False, False, False,  True, False, False,\n         False,  True, False, False, False, False, False, False, False, False,\n         False, False,  True, False, False, False, False, False, False,  True,\n          True, False]])"},"metadata":{}}]},{"cell_type":"markdown","source":"Getting indices to be masked","metadata":{}},{"cell_type":"code","source":"# create selection from mask_arr\nselection = torch.flatten((mask_arr[0]).nonzero()).tolist()\nselection","metadata":{"execution":{"iopub.status.busy":"2023-05-22T17:47:37.887707Z","iopub.execute_input":"2023-05-22T17:47:37.888696Z","iopub.status.idle":"2023-05-22T17:47:37.895801Z","shell.execute_reply.started":"2023-05-22T17:47:37.888659Z","shell.execute_reply":"2023-05-22T17:47:37.894812Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"[12, 13, 14, 19, 21, 24, 25, 37, 41, 52, 59, 60]"},"metadata":{}}]},{"cell_type":"markdown","source":"masking","metadata":{}},{"cell_type":"code","source":"inputs.input_ids[0, selection] = 103 # mask token = 103","metadata":{"execution":{"iopub.status.busy":"2023-05-22T17:47:38.021736Z","iopub.execute_input":"2023-05-22T17:47:38.022107Z","iopub.status.idle":"2023-05-22T17:47:38.027750Z","shell.execute_reply.started":"2023-05-22T17:47:38.022079Z","shell.execute_reply":"2023-05-22T17:47:38.026790Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"inputs.input_ids","metadata":{"execution":{"iopub.status.busy":"2023-05-22T17:47:38.404744Z","iopub.execute_input":"2023-05-22T17:47:38.405700Z","iopub.status.idle":"2023-05-22T17:47:38.413470Z","shell.execute_reply.started":"2023-05-22T17:47:38.405666Z","shell.execute_reply":"2023-05-22T17:47:38.412370Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"tensor([[  101,  2044,  8181,  5367,  2180,  1996,  2281,  7313,  4883,  2602,\n          2006,  2019,   103,   103,   103,  4132,  1010,  2019,  3988,   103,\n          6658,   103,  4161,  2037,   103,   103,  1996,  2406,  2000,  2433,\n          1996, 18179,  1012,  2162,  3631,  2041,  1999,   103,  6863,  2043,\n         22965,   103,  2749,  4457,  3481,  7680,  3334,  1999,  2148,  3792,\n          1010,  2074,   103,  1037,  3204,  2044,  5367,  1005,  1055,   103,\n           103,   102]])"},"metadata":{}}]},{"cell_type":"code","source":"outputs = model(**inputs)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T17:47:38.956888Z","iopub.execute_input":"2023-05-22T17:47:38.957569Z","iopub.status.idle":"2023-05-22T17:47:39.854280Z","shell.execute_reply.started":"2023-05-22T17:47:38.957534Z","shell.execute_reply":"2023-05-22T17:47:39.852668Z"},"trusted":true},"execution_count":49,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1358\u001b[0m, in \u001b[0;36mBertForMaskedLM.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1349\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1350\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1351\u001b[0m \u001b[38;5;124;03m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[1;32m   1352\u001b[0m \u001b[38;5;124;03m    config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\u001b[39;00m\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;124;03m    loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1354\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1356\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1358\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1364\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1367\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1368\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1370\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1372\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1373\u001b[0m prediction_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls(sequence_output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1013\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1020\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m   1021\u001b[0m     embedding_output,\n\u001b[1;32m   1022\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1031\u001b[0m )\n\u001b[1;32m   1032\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:230\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    227\u001b[0m         token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 230\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m token_type_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_type_embeddings(token_type_ids)\n\u001b[1;32m    233\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m token_type_embeddings\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"],"ename":"RuntimeError","evalue":"Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)","output_type":"error"}]},{"cell_type":"code","source":"outputs.keys()","metadata":{"execution":{"iopub.status.busy":"2023-05-22T17:47:39.855365Z","iopub.status.idle":"2023-05-22T17:47:39.856416Z","shell.execute_reply.started":"2023-05-22T17:47:39.856157Z","shell.execute_reply":"2023-05-22T17:47:39.856183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outputs.loss","metadata":{"execution":{"iopub.status.busy":"2023-05-22T17:47:40.131996Z","iopub.execute_input":"2023-05-22T17:47:40.132342Z","iopub.status.idle":"2023-05-22T17:47:40.140011Z","shell.execute_reply.started":"2023-05-22T17:47:40.132312Z","shell.execute_reply":"2023-05-22T17:47:40.138982Z"},"trusted":true},"execution_count":50,"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"tensor(0.7097, grad_fn=<NllLossBackward0>)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"from tqdm import notebook\nfrom nltk.tokenize import RegexpTokenizer\nimport re","metadata":{"execution":{"iopub.status.busy":"2023-05-22T17:47:47.057242Z","iopub.execute_input":"2023-05-22T17:47:47.057600Z","iopub.status.idle":"2023-05-22T17:47:47.062158Z","shell.execute_reply.started":"2023-05-22T17:47:47.057569Z","shell.execute_reply":"2023-05-22T17:47:47.061058Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"def preprocess(sentence):\n    sentence=str(sentence)\n    sentence = sentence.lower()\n    sentence=sentence.replace('{html}',\"\") \n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr, '', sentence)\n    cleantext = re.sub(r'http\\S+', '',cleantext)\n    return cleantext","metadata":{"execution":{"iopub.status.busy":"2023-05-22T17:47:47.410999Z","iopub.execute_input":"2023-05-22T17:47:47.411392Z","iopub.status.idle":"2023-05-22T17:47:47.417955Z","shell.execute_reply.started":"2023-05-22T17:47:47.411364Z","shell.execute_reply":"2023-05-22T17:47:47.416012Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"def clean(sentences):\n    cleaned_sentences = []\n    for sentence in notebook.tqdm(sentences):\n        cleaned_sentences.append(preprocess(sentence))\n    return cleaned_sentences","metadata":{"execution":{"iopub.status.busy":"2023-05-22T17:47:49.755796Z","iopub.execute_input":"2023-05-22T17:47:49.756364Z","iopub.status.idle":"2023-05-22T17:47:49.763092Z","shell.execute_reply.started":"2023-05-22T17:47:49.756331Z","shell.execute_reply":"2023-05-22T17:47:49.762081Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"text = clean(comments)\ntext[:5]","metadata":{"execution":{"iopub.status.busy":"2023-05-22T17:47:50.209043Z","iopub.execute_input":"2023-05-22T17:47:50.209406Z","iopub.status.idle":"2023-05-22T17:47:53.792069Z","shell.execute_reply.started":"2023-05-22T17:47:50.209376Z","shell.execute_reply":"2023-05-22T17:47:53.791008Z"},"trusted":true},"execution_count":54,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/622630 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ab80863c06f4d2ea3c5a99dd5357db8"}},"metadata":{}},{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"[\"it's more accurate to call it the m+ (1000) because the price is closer than calling it the x (10).\",\n 'to be there with a samsung phone\\\\nüòÇüòÇüòÇ',\n 'thank gosh, a place i can watch it without having to be at hd... my speed doesn‚Äôt support hd',\n 'what happened to the home button on the iphone x? *****cough****copying samsung******cough',\n 'power is the disease.\\xa0 care is the cure.\\xa0 keep caring for yourself and others as best as you can.\\xa0 this is life.']"},"metadata":{}}]},{"cell_type":"code","source":"inputs = tokenizer(text, return_tensors='pt', max_length=256, truncation=True, padding='max_length')","metadata":{"execution":{"iopub.status.busy":"2023-05-22T17:47:54.557641Z","iopub.execute_input":"2023-05-22T17:47:54.558608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}